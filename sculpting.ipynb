{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81efb18b-2469-4c78-9113-1289cba7a310",
   "metadata": {},
   "source": [
    "# Manifold Sculpting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7288a54d-7e95-4ab4-a486-6a009644360b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d42a487-2cbe-4a7a-b6b6-b8d4f808bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Dataset (MNIST)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                             \n",
    "])\n",
    "\n",
    "# train_test_data = torchvision.datasets.MNIST(root='./Data', train=True, download=True, transform=transform)\n",
    "train_data = torchvision.datasets.MNIST(root='./Data', train=True, download=True, transform=transform)\n",
    "val_data  = torchvision.datasets.MNIST(root='./Data', train=False, download=True, transform=transform)\n",
    "\n",
    "#frac = 0.98\n",
    "#train_data, test_data = torch.utils.data.random_split(train_test_data, [frac, 1-frac], generator=torch.Generator().manual_seed(42))\n",
    "print(len(train_data))\n",
    "print(len(  val_data))\n",
    "#print(len( test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c707943e-8d07-4dff-9060-6512709af8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out images with labels 1 and 7 from the training dataset\n",
    "indices = (train_data.targets == 1) + (train_data.targets == 2) # this has the same length as train_data.targets, filled with 1 and 0s\n",
    "indices[8000:] = False  # Limit the number of images with labels 1 and 7\n",
    "train_data.data, train_data.targets = train_data.data[indices], train_data.targets[indices]\n",
    "\n",
    "# Same thing for the Validation dataset\n",
    "indices = (val_data.targets == 1) + (val_data.targets == 2)\n",
    "indices[4000:] = False\n",
    "val_data.data, val_data.targets = val_data.data[indices], val_data.targets[indices]\n",
    "\n",
    "# Same thing for the Test dataset\n",
    "#indices = (test_data.targets == 1) + (test_data.targets == 7)\n",
    "#indices[4000:] = False\n",
    "#test_data.data, test_data.targets = test_data.data[indices], test_data.targets[indices]\n",
    "\n",
    "print(\"Train set: \", len(train_data.targets), \"samples \\nVal   set: \", len(val_data.targets), \"samples\")\n",
    "#print(\"Train set: \", len(train_data.targets), \"samples \\nVal set: \", len(val_data.targets), \"samples \\nTest set: \", len(test_data.targets), \"samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a62a487-093c-4f83-a311-678bd7339593",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=len(train_data), shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4b5e6f-1626-4d4c-8fba-8565e05da692",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Programma dla mattina:\n",
    "- [x] Parti leggendo il pezzo che manca dl paper\n",
    "- [x] CONTROLLA che qlo che hai già scritto funzioni \n",
    "    (cfr con il bro per il senso generale, che le funzioni facciano qlo che ti aspetti con dle test cells)\n",
    "- [x] bisogna finire fit\n",
    "- [x] scrivere adjust point\n",
    "- [x] organizzare tutto in 1 classe\n",
    "\n",
    "## Poi incrociare forte le dita pké funzioni su MNIST\n",
    "\n",
    "- [ ] Genera la manifold con il vae (ridotto a 1 o 2 cifre) e già puoi vedere se le organizzano in modi molto #i\n",
    "- [ ] Prova con 2 cifre (ma qua stiamo già azzardando)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d153b71-f67d-4891-828d-dee587fb29bf",
   "metadata": {},
   "source": [
    "### Test cells"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4eec3c27-ece7-4562-9443-246d53f00962",
   "metadata": {},
   "source": [
    "Dumb way\n",
    "# troviamo i 1i k vicini\n",
    "scarti = p*torch.ones_like(data) - data\n",
    "dist_sq = scarti*scarti\n",
    "dist_sq = dist_sq.sum(dim=p_size_idx)\n",
    "\n",
    "# sort the points\n",
    "dist_sq, indices = torch.sort(dist_sq)\n",
    "print(dist_sq)\n",
    "# keep the first k (without the point itself)\n",
    "kneighb = indices[1:k+1]\n",
    "dist = torch.sqrt(dist_sq[1:k+1])\n",
    "\n",
    "print(kneighb)\n",
    "print(dist_sq[1:k+1])\n",
    "print(dist)\n",
    "# tutto qsto va in 1 ciclo sui .i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7619254b-b03a-4370-a3fd-66fab2f6241e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6)\n",
      "tensor([2])\n"
     ]
    }
   ],
   "source": [
    "k = 2\n",
    "data = torch.Tensor([[1,1],[7,7],[9,9],[12,12], [5,5], [2,2]])\n",
    "p = data[2]\n",
    "sigma = 0.1\n",
    "n_dim = 1\n",
    "scale_factor = 1 # at first\n",
    "\n",
    "data_size = torch.tensor(data.size())\n",
    "m = data_size[0] # number of data points\n",
    "p_size = data_size[1:] # size of the single datapoint\n",
    "p_size0 = p_size - torch.ones_like(p_size) # to be used for indexing\n",
    "print(m)\n",
    "print(p_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0a07a21b-14ad-4252-aa1b-b12f3cd976d1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test cell\n",
    "a = torch.Tensor([1,2])\n",
    "b = torch.Tensor([2,3])\n",
    "c = a@b\n",
    "d = c.item()\n",
    "a.tolist()\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3271f0a4-bd63-47f4-9f52-e61e09f1c3cd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7071, 0.7071])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.7854)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test cell\n",
    "from math import acos\n",
    "a = torch.Tensor([0,1])\n",
    "b = torch.Tensor([1,1])\n",
    "a /= torch.norm(a)\n",
    "b /= torch.norm(b)\n",
    "print(b)\n",
    "c = torch.acos(a@b)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "134da2b7-f53a-4f60-a2e9-d7b3369eba21",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.3028, 3.0000, 0.6972])\n",
      "tensor([[ 0.2898,  0.0000, -0.9571],\n",
      "        [ 0.9571,  0.0000,  0.2898],\n",
      "        [ 0.0000,  1.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "## Test cell\n",
    "a = torch.Tensor([[1,1,0],\n",
    "                  [1,4,0],\n",
    "                  [0,0,3]])\n",
    "eigenval, eigenvec = torch.linalg.eigh(-a)\n",
    "eigenval = -eigenval\n",
    "print(eigenval)\n",
    "print(eigenvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ad665670-05be-456e-acce-f624e3b566be",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "che tavanata\n",
      "ush\n",
      "ush\n"
     ]
    }
   ],
   "source": [
    "## Test cell\n",
    "a = [1,2]\n",
    "if [0]:\n",
    "    print(\"che tavanata\")\n",
    "while a:\n",
    "    a.pop(0)\n",
    "    print(\"ush\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8657c5fe-44e5-4d26-9583-7e2ca09d17c3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2],\n",
      "        [3]])\n",
      "tensor([3.])\n",
      "tensor([8.])\n"
     ]
    }
   ],
   "source": [
    "## Test cell\n",
    "a = torch.Tensor([[1,2,3,4],[5,6,7,8]])\n",
    "indx = torch.multinomial(a, num_samples=1) # replacement?\n",
    "print(indx)\n",
    "print(a[0,indx[0]])\n",
    "print(a[1,indx[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cf11c6d6-2af8-4ab4-b70f-fa727cf66947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3728734254837036"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+2*torch.rand(1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1ec2d4cd-13bd-4ac7-8298-77f0c76de053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.,  9.])\n",
      "tensor([[ 0.4472, -0.8944],\n",
      "        [-0.8944, -0.4472]])\n",
      "tensor([[ 1., -2.],\n",
      "        [-2., -1.]])\n",
      "tensor([[-0.4472,  0.8944],\n",
      "        [-8.0498, -4.0249]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([[7,4],\n",
    "                  [4,1]])\n",
    "from math import sqrt\n",
    "eigenval, eigenvec = torch.linalg.eigh(a)\n",
    "# me li restituisce già in base ON\n",
    "# verdetto finale: NN ci vuole ness1 trasposizione di sorta\n",
    "print(eigenval)\n",
    "print(eigenvec)\n",
    "print(eigenvec*sqrt(5))\n",
    "pca_basis = eigenvec@a\n",
    "print(pca_basis)\n",
    "\n",
    "#       cov = torch.cov(self.data)\n",
    "#       eigenval, eigenvec = torch.linalg.eigh(-cov) # Li ordina già in vista dla pca!\n",
    "#       eigenval = -eigenval  # Pké li ordina decrescenti\n",
    "#       pca_data = eigenvec@self.data # È qla giusta o è qla trasposta?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1c864c4-10a2-4b57-bb51-3f26ae4d92f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (1, 4), (2, 2), (3, 7), (4, 1), (5, 6)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [0,4,2,7,1,6]\n",
    "list(enumerate(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8816805e-904c-4860-a696-e13326093959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.],\n",
      "        [17.,  2.]])\n",
      "tensor([[1., 2.],\n",
      "        [1., 2.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([[1,2],[1,2]])\n",
    "len(a.size())\n",
    "b = torch.clone(a)\n",
    "a[1,0] = 17\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8fc0aada-70cf-46bc-9b58-f36eaed443d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1.])\n",
      "tensor([0., 1.])\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2)\n",
    "b = torch.tensor([0,1])\n",
    "c = torch.tensor([0,1])\n",
    "print(a)\n",
    "a[0] = torch.acos(b@c)\n",
    "print(a)\n",
    "print(torch.acos(b@c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05478780-224e-47c0-9231-67f1e4b13af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(20.)\n",
      "tensor(2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([1,5,20,3])\n",
    "b, index = torch.max(a, dim=0)\n",
    "print(b)\n",
    "print(index)\n",
    "b /= b\n",
    "torch.acos(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "06022ca1-56b3-47cc-9c8c-006867ea2e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "1694\n",
      "1694\n",
      "torch.Size([1, 28, 28])\n",
      "tensor(0.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=len(train_data), shuffle=True)\n",
    "for _ in train_loader:\n",
    "    print(type(_))\n",
    "    print(len(_[0])) # qste sono tutte le foto\n",
    "    print(len(_[1])) # qsti sono i targets\n",
    "    print(_[0][0].size())   # qsto è il torch.tensor con la singola immagine \n",
    "    # size = 1, 28, 28 dove 1 è il channel\n",
    "    print(_[0][0][0,0,0])   # qsto è il tensor con la singola immagine \n",
    "    _[0][0][0,0,0] = 1\n",
    "    print(_[0][0][0,0,0])   # qsto è il tensor con la singola immagine "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccf400f-8e27-427b-a89e-8ba750601c46",
   "metadata": {},
   "source": [
    "# Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "58a9e7c7-6e3e-4222-bb3d-fae30d3a1ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManifoldSculpting():\n",
    "    ''' \n",
    "    Dependencies: import torch\n",
    "    '''\n",
    "    def __init__(self, k=5, n_dim=2, niter=100, sigma=0.98, patience=20): # rotate = True\n",
    "        # Implementation of the Manifold Sculpting algorithm in PyTorch\n",
    "\n",
    "        # Hyperparameters of the algorithm\n",
    "        # Only torch.tensor()\n",
    "        self.k = k                    # -- number of neighbors considered\n",
    "        self.n_dim = n_dim            # -- dimension of the searched manifold\n",
    "        self.niter = niter            # -- 1st stopping criterion for the iterative cicle \n",
    "        self.sigma = sigma            # -- scaling factor at each iteration (for extra dimensions)\n",
    "        self.scale_factor = 1         # -- cumulative scale factor\n",
    "        self.patience = patience      # -- 2nd stopping criterion\n",
    "\n",
    "    def transform(self, data):\n",
    "        ####### MAIN #######\n",
    "\n",
    "        ## ---- Import\n",
    "        # In case of images or weirdly shaped input points\n",
    "        if len(data.size()) > 2:\n",
    "            flatten   = torch.nn.Flatten()\n",
    "            self.data = flatten(data) \n",
    "            # self.original_p_size = data.size[1:] # superfluo\n",
    "            #print(\"Flattened\")\n",
    "        else:\n",
    "            self.data = data\n",
    "            \n",
    "        self.p_size = self.data.size()[1]   # single point flattenend dimension \n",
    "        self.n_datapoints = data.size()[0]  # int\n",
    "\n",
    "        ## ---- Compute neighbors relations\n",
    "        self.dist, self.neighb    = self.neighb_distance()\n",
    "        self.colinear, self.theta = self.colinear_neighb()\n",
    "        print('''\n",
    "        INFO: Neighbor relations computed\n",
    "        ''')\n",
    "        \n",
    "        self.avg_dist = torch.mean(self.dist)\n",
    "\n",
    "        self.nudge = self.avg_dist\n",
    "\n",
    "        ## ---- PCA transform\n",
    "        self.data = self.pca_transform()\n",
    "        \n",
    "        # Distinguish dimensions to be scaled/preserved\n",
    "        self.preserv_dim = torch.tensor(list(range(self.n_dim)))\n",
    "        self.scaled_dim  = torch.tensor(list(range(self.n_dim, self.p_size)))\n",
    "\n",
    "        ## ---- Iterative transformation\n",
    "        epoch = 1\n",
    "        print('''\n",
    "        INFO: Starting preliminary adjustments (NO error comparison)\n",
    "        ''')\n",
    "        \n",
    "        # Adjust a bunch of times without comparing errors\n",
    "        while self.scale_factor > 0.01: # can be tuned\n",
    "            mean_error = self.step()\n",
    "            epoch += 1\n",
    "\n",
    "        epochs_since_improvement = 0\n",
    "        best_error = torch.Tensor(float('inf'))\n",
    "        print('''\n",
    "        INFO: First round of adjustments finished \n",
    "        INFO: Start comparing errors (stopping criteria: niter, patience)\n",
    "        ''')\n",
    "\n",
    "        # Continue adjusting, start comparing errors\n",
    "        while (epoch < self.niter) and (epochs_since_improvement < self.patience):\n",
    "            mean_error = self.step()\n",
    "\n",
    "            if mean_error < best_error:\n",
    "                best_error = mean_error\n",
    "                self.best_data  = torch.clone(self.data)\n",
    "                self.best_error = best_error\n",
    "                epochs_since_improvement = 0\n",
    "            else:\n",
    "                epochs_since_improvement += 1\n",
    "                \n",
    "            epochs += 1\n",
    "            if epochs%20:\n",
    "                print(f'''\n",
    "                INFO: Elapsed epochs: {epochs}''')\n",
    "\n",
    "        ## DEBUG and monitoring\n",
    "        self.elapsed_epochs = epoch\n",
    "        self.last_error = mean_error\n",
    "\n",
    "\n",
    "\n",
    "    def neighb_distance(self):\n",
    "        '''\n",
    "        Returns:\n",
    "        - tensor.size() = (n_datapoints, k)\n",
    "          (i,j) hosts the DISTANCE of point i to its j-th neighbor \n",
    "          with distances decreasing in j\n",
    "        - tensor.size() = (n_datapoints, k) \n",
    "          (i,j) hosts the INDEX (relative to self.data) of the j-th neighbor of point i\n",
    "        '''\n",
    "        x2 = self.data * self.data\n",
    "        x2 = x2.sum(axis=1)\n",
    "        data_t = torch.transpose(self.data, 0, 1) # pb in dim >i (es foto...), credo funzioni cmq\n",
    "        xx = self.data@data_t\n",
    "        \n",
    "        all_distances = torch.sqrt( x2 - 2*xx + x2.unsqueeze(dim=1) ) # they have different dimensions, we leverage pytorch default for the operations\n",
    "        \n",
    "        _, indices = torch.sort(all_distances)\n",
    "\n",
    "        kneighb = indices[:, 1:self.k+1]\n",
    "        #all_distances   = torch.zeros_like(kneighb)\n",
    "\n",
    "        # Cherry-pick neighbors distances (sorted)\n",
    "        d = []  # For some reason it doesn't allow directly with tensors\n",
    "        for i in range(self.n_datapoints):\n",
    "            #print(kneighb[i,:])\n",
    "            #print(all_distances.size())\n",
    "            d.append( all_distances[kneighb[i,:], i] )\n",
    "        \n",
    "        kdist = torch.reshape( torch.cat( d, 0 ), (-1, self.k) )\n",
    "        \n",
    "        return kdist, kneighb\n",
    "\n",
    "    \n",
    "    def avg_neighb_distance(self):\n",
    "        dist, _ = self.neighb_distance()\n",
    "        avg_dist = torch.mean(dist)\n",
    "        \n",
    "        return avg_dist\n",
    "\n",
    "    def colinear_neighb(self):\n",
    "        '''\n",
    "        For clarity:\n",
    "        Variables with _idx  have values in (0, m-1) (selecting points from self.data)\n",
    "        Variables with _kidx have values in (0, k-1)\n",
    "\n",
    "        Returns:\n",
    "        - tensor.size() = (n_datapoints, k)\n",
    "        (i,j) hosts the ANGLE theta (i-j-l) of the most colinear point l to the couple i-j\n",
    "        - tensor.size() = (n_datapoints, k)\n",
    "        (i,j) hosts the NEIGHBOR INDEX of l with respect to the neighbourhood of j\n",
    "        '''\n",
    "        theta    = torch.ones((self.n_datapoints,self.k))\n",
    "        colinear = torch.ones((self.n_datapoints,self.k))\n",
    "        \n",
    "        # Loop over data points\n",
    "        for i_idx in range(self.n_datapoints):\n",
    "            # Loop over neighbors of i\n",
    "            for j_kidx, j_idx in enumerate(self.neighb[i_idx]):\n",
    "                \n",
    "                p2j = self.data[i_idx,:] - self.data[j_idx,:]\n",
    "                #print(self.data[i_idx,:])\n",
    "                #print(p2j.size())\n",
    "                #print(p2j)\n",
    "                p2j /= torch.norm(p2j)\n",
    "        \n",
    "                colinear_kidx =  torch.ones(self.k)\n",
    "                cos_i2l = torch.ones(self.k)\n",
    "                # Loop over neighbor points of j\n",
    "                for l_kidx, l_idx in enumerate(self.neighb[j_idx]):\n",
    "                    \n",
    "                    p2l = self.data[l_idx] - self.data[j_idx]\n",
    "                    p2l /= torch.norm(p2l)\n",
    "                    cos_i2l[l_kidx] = p2l@p2j\n",
    "                    \n",
    "                # Extract the colinear angle and neighbor\n",
    "                cos_max, colinear[i_idx, j_kidx] = torch.max(cos_i2l, dim=0)\n",
    "                colinear = colinear.int()\n",
    "                theta[i_idx, j_kidx] = torch.acos(cos_max)\n",
    "\n",
    "        return colinear, theta\n",
    "    \n",
    "            \n",
    "    def pca_transform(self):\n",
    "        '''\n",
    "        Returns\n",
    "        - tensor.size() = (n_datapoints, p_size)\n",
    "          the data linearly transformed to the principal components basis\n",
    "        '''\n",
    "        cov = torch.cov(self.data)\n",
    "        eigenval, eigenvec = torch.linalg.eigh(-cov) # -cov because it outputs sorted descending eigenval\n",
    "        eigenval = -eigenval\n",
    "        pca_data = eigenvec@self.data\n",
    "        \n",
    "        return pca_data\n",
    "        \n",
    "    \n",
    "    def compute_error(self, p, visited):\n",
    "        '''\n",
    "        Parameters:\n",
    "        - p : (int)         index of the point\n",
    "        - visited: (list)   list of already adjusted points\n",
    "        \n",
    "        Returs:\n",
    "        - (float) error relative to the neighbourhood of p\n",
    "        '''\n",
    "        \n",
    "        w = torch.ones(self.k)\n",
    "        for j in range(self.k):\n",
    "            w[j] = 10 if self.neighb[p,j] in visited else 1\n",
    "    \n",
    "        total_err = 0\n",
    "        for i in range(self.k):\n",
    "            # Extract indices\n",
    "            n = self.neighb[p,i].item()\n",
    "            c = self.colinear[p,i].item()\n",
    "\n",
    "            # Compute theta_p2c, the angle in p-n-c\n",
    "            p2n = self.data[p] - self.data[n]\n",
    "            c2n = self.data[c] - self.data[n]\n",
    "            p2n /= torch.norm(p2n)\n",
    "            c2n /= torch.norm(c2n)\n",
    "            theta_p2c = torch.acos(p2n@c2n)\n",
    "\n",
    "            # Compute error\n",
    "            err_dist = .5*(torch.norm(p2n) - self.dist[p,i]) / self.avg_dist\n",
    "            err_theta = (theta_p2c - self.theta[p,i])/3.1415926535\n",
    "            \n",
    "            total_err += w[i] * (err_dist*err_dist + err_theta*err_theta)\n",
    "            \n",
    "        return total_err\n",
    "    \n",
    "    \n",
    "    def adjust_point(self, p, visited):\n",
    "        '''\n",
    "        Parameters:\n",
    "        - p: (int) index of the point to be adjusted\n",
    "        - visited: (list) list of already adjusted points in current epoch\n",
    "\n",
    "        Returns:\n",
    "        - number of hill descent steps\n",
    "        - error for the adjusted point\n",
    "        '''\n",
    "        # Slightly randomize the entity of the update\n",
    "        nudge = self.nudge * ( .6 + .4*torch.rand(1).item() ) # float\n",
    "        \n",
    "        s = -1 \n",
    "        improved = True\n",
    "        err = self.compute_error(p, visited)\n",
    "        \n",
    "        while (s<30) and improved:\n",
    "            s += 1\n",
    "            improved = False\n",
    "\n",
    "            ## --- Downhill update\n",
    "            # Loop over dimensions (try the same nudge for all of them)\n",
    "            for d in self.preserv_dim:\n",
    "                \n",
    "                self.data[p,d] += nudge  # Try one direction\n",
    "                new_err = self.compute_error(p, visited)\n",
    "\n",
    "                if new_err >= err:\n",
    "                    self.data[p,d] -= 2*nudge  # Try in the opposite\n",
    "                    new_err = self.compute_error(p, visited)\n",
    "                    \n",
    "                if new_err >= err:\n",
    "                    self.data[p,d] += nudge # Stay put\n",
    "                    \n",
    "                else:\n",
    "                    err = new_err\n",
    "                    improved = True\n",
    "                    \n",
    "        return s, err\n",
    "\n",
    "\n",
    "    def step(self):\n",
    "        '''\n",
    "        Returns:\n",
    "        - (float) mean error after adjustment\n",
    "        '''\n",
    "        # ---- a)\n",
    "        # (a,b refer to pseudo-code (Fig 2.2) in original paper)\n",
    "        \n",
    "        self.scale_factor *= self.sigma\n",
    "        \n",
    "        # Downscale component along scaled dimensions\n",
    "        self.data[:, self.scaled_dim] *= self.sigma\n",
    "    \n",
    "        # Upscale the component along preserved dimensions\n",
    "        while (self.avg_neighb_distance() < self.avg_dist): # mi sfugge li senso di qsto criterio\n",
    "            self.data[:, self.preserv_dim] /= self.sigma\n",
    "            \n",
    "        # ---- b)\n",
    "        pr_idx = torch.multinomial(torch.ones(self.n_datapoints), num_samples=1)\n",
    "        # pr = data[rand_indx,:]\n",
    "\n",
    "        queue_idx = []\n",
    "        queue_idx.append(pr_idx.item())\n",
    "        visited = []\n",
    "    \n",
    "        step = 0\n",
    "        mean_error = 0\n",
    "        counter = 0\n",
    "        \n",
    "        # while the queue is not empty\n",
    "        while queue_idx:\n",
    "            p = queue_idx.pop(0)\n",
    "            if p in visited:\n",
    "                continue\n",
    "    \n",
    "            # Add p's neighbors in the queue\n",
    "            for n in self.neighb[p]:\n",
    "                queue_idx.append(n.item())\n",
    "                \n",
    "            s, err = self.adjust_point(p,visited) \n",
    "    \n",
    "            step += s\n",
    "            mean_error += err\n",
    "            counter += 1\n",
    "            visited.append(p)\n",
    "    \n",
    "        mean_error /= counter\n",
    "    \n",
    "        # numbers from author's implementation (weight decay-like)\n",
    "        if step < self.n_datapoints:\n",
    "            self.nudge *= 0.87\n",
    "        else:\n",
    "            self.nudge /= 0.91\n",
    "            \n",
    "        return mean_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31d66e4-fee7-41e0-a862-0790027ad65c",
   "metadata": {},
   "source": [
    "# Attenti al lupo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a50e81ea-6b09-4b22-8fa2-21ce49e386b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[127], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m sculptor \u001b[38;5;241m=\u001b[39m ManifoldSculpting(k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m) \u001b[38;5;66;03m# defaults: k=5, n_dim=2, niter=100, sigma=0.99, patience=30\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m----> 3\u001b[0m     \u001b[43msculptor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m \u001b[38;5;66;03m# superfluo pké c'è 1 solo elemento\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpochs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msculptor\u001b[38;5;241m.\u001b[39melapsed_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[126], line 53\u001b[0m, in \u001b[0;36mManifoldSculpting.transform\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Adjust a bunch of times without comparing errors\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_factor \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.01\u001b[39m: \u001b[38;5;66;03m# can be tuned\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m     mean_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     56\u001b[0m epochs_since_improvement \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[126], line 292\u001b[0m, in \u001b[0;36mManifoldSculpting.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneighb[p]:\n\u001b[1;32m    290\u001b[0m     queue_idx\u001b[38;5;241m.\u001b[39mappend(n\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m--> 292\u001b[0m s, err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjust_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvisited\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    294\u001b[0m step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m s\n\u001b[1;32m    295\u001b[0m mean_error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m err\n",
      "Cell \u001b[0;32mIn[126], line 237\u001b[0m, in \u001b[0;36mManifoldSculpting.adjust_point\u001b[0;34m(self, p, visited)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreserv_dim:\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[p,d] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m nudge  \u001b[38;5;66;03m# Try one direction\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m     new_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisited\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_err \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m err:\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[p,d] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mnudge  \u001b[38;5;66;03m# Try in the opposite\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[126], line 187\u001b[0m, in \u001b[0;36mManifoldSculpting.compute_error\u001b[0;34m(self, p, visited)\u001b[0m\n\u001b[1;32m    185\u001b[0m w \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk):\n\u001b[0;32m--> 187\u001b[0m     w[j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneighb[p,j] \u001b[38;5;129;01min\u001b[39;00m visited \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    189\u001b[0m total_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk):\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;66;03m# Extract indices\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sculptor = ManifoldSculpting(k=10, sigma=.97) # defaults: k=5, n_dim=2, niter=100, sigma=0.98, patience=20\n",
    "for data, labels in train_loader:\n",
    "    sculptor.transform(data)\n",
    "    break # superfluo pké c'è 1 solo elemento\n",
    "    \n",
    "print(f'Epochs: {sculptor.elapsed_epochs}')\n",
    "print(f'Final mean error: {scupltor.last_error}')\n",
    "print(f'Best mean error: {sculptor.best_error}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
